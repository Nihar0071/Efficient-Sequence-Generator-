# Efficient-Sequence-Generator-
The File in the repository represnts the work of sequence generating transformer which works on character level text generation 
with a reduced loss of 1.79 with less number of parameters for working on versatile data.
# Federated learning 
The model is integrated with federated learning for adapting continuosly to for optimising the global model which does not have any access to data which is deventralised.
# Model weights averaging and parameter sharing 
The global model reduces losses by simple federated averaging of local model weights over a small number of itertions.
